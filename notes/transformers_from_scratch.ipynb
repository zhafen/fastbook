{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers from Scratch\n",
    "This is a workthrough of [the nice derivation found here.](https://e2eml.school/transformers.html#table_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Words -> numerical representation.\n",
    "\n",
    "Consider \"find my files\":\n",
    "\n",
    "\\begin{matrix}\n",
    "\\rm{files} & \\rm{find} & \\rm{my}\n",
    "\\end{matrix}\n",
    "\n",
    "\\begin{pmatrix}\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "1 & 0 & 0\n",
    "\\end{pmatrix}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas one-hot encoding\n",
    "phrase = pd.Series(['find', 'my', 'files'])\n",
    "encoded_phrase = pd.get_dummies(phrase) # one-hot encoding\n",
    "encoded_phrase.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Dot product\n",
    "\n",
    "$\\vec{a} \\cdot \\vec{b} \\equiv a_i b_i = c$ (scalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Matrix multiplication\n",
    "\n",
    "$\\bf{A} \\bf{B} \\equiv \\rm{A_{ij}B_{jk}} = C_{ik}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Matrix multiplication as a table lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_matrix = rng.uniform(size=(3, 3))\n",
    "some_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_vec = encoded_phrase['files']\n",
    "np.matmul(some_matrix, one_hot_vec) # Last column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. First order sequence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = [\n",
    "    'show me my directories please',\n",
    "    'show me my files please',\n",
    "    'show me my photos please',\n",
    "]\n",
    "tokenized_phrases = [simple_tokenize(phrase) for phrase in phrases]\n",
    "tokenized_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(tokenized_phrases):\n",
    "\n",
    "    # Get all tokens\n",
    "    tokens = set()\n",
    "    for tokens_i in tokenized_phrases:\n",
    "        tokens.update(tokens_i)\n",
    "\n",
    "    # Convert to dictionary\n",
    "    vocab = {}\n",
    "    for i, token in enumerate(sorted(tokens)):\n",
    "        vocab[token] = i\n",
    "\n",
    "    return pd.Series(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(tokenized_phrases)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transition_df(tokenized_phrases, vocab, h=1, verbose=True):\n",
    "\n",
    "    transition_mat = np.zeros((len(vocab), len(vocab))).astype(int)\n",
    "\n",
    "    # Loop through and add\n",
    "    for ii, tokens_ii in enumerate(tokenized_phrases):\n",
    "        for jj, word in enumerate(tokens_ii):\n",
    "            start = max(0, jj - h)\n",
    "            for k, other_word in enumerate(tokens_ii[start:jj]):\n",
    "\n",
    "                i = vocab.loc[other_word]\n",
    "                j = vocab.loc[word]\n",
    "\n",
    "                if verbose:\n",
    "                    print(f'({start}, {jj}): {other_word} -> {word} @ {i}, {j}')\n",
    "\n",
    "                transition_mat[i, j] += 1\n",
    "\n",
    "    transition_df = pd.DataFrame(\n",
    "        transition_mat,\n",
    "        index=vocab.index,\n",
    "        columns=vocab.index\n",
    "    )\n",
    "\n",
    "    return transition_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_df = build_transition_df(tokenized_phrases, vocab)\n",
    "transition_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilities\n",
    "transition_df.div(transition_df.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_to_one_hot(vocab):\n",
    "    mat = np.identity(len(vocab), dtype=int)\n",
    "    return pd.DataFrame(mat, index=vocab.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_identity = vocab_to_one_hot(vocab)\n",
    "vocab_identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(transition_df.T.values, vocab_identity.loc['my'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Second order sequence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = [\n",
    "    'check whether the battery ran down please',\n",
    "    'check whether the program ran please'\n",
    "]\n",
    "tokenized_phrases = [simple_tokenize(phrase) for phrase in phrases]\n",
    "tokenized_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ngram_vocab(phrases, *args, **kwargs):\n",
    "\n",
    "    vectorizer = CountVectorizer(*args, **kwargs)\n",
    "    X = vectorizer.fit_transform(phrases)\n",
    "    vectorized_phrases = pd.DataFrame(\n",
    "        X.toarray(),\n",
    "        columns=vectorizer.get_feature_names_out(),\n",
    "    )\n",
    "\n",
    "    vocab = pd.Series(vectorizer.vocabulary_).sort_values()\n",
    "\n",
    "    return vectorized_phrases, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phrases, bigram_vocab = build_ngram_vocab(phrases, ngram_range=(2, 2))\n",
    "bigram_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(tokenized_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, bigram_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_direct_bigram_transition_df(\n",
    "    tokenized_phrases,\n",
    "    vocab,\n",
    "    bigram_vocab,\n",
    "    verbose=True,\n",
    "):\n",
    "\n",
    "    transition_mat = np.zeros((len(bigram_vocab), len(vocab))).astype(int)\n",
    "\n",
    "    # Loop through and add\n",
    "    for ii, tokens_ii in enumerate(tokenized_phrases):\n",
    "        for jj, word in enumerate(tokens_ii):\n",
    "\n",
    "            # No bigram for first word\n",
    "            if jj < 2:\n",
    "                continue\n",
    "\n",
    "            preceeding_bigram = ' '.join([tokens_ii[jj - 2], tokens_ii[jj - 1]])\n",
    "\n",
    "            i = bigram_vocab.loc[preceeding_bigram]\n",
    "            j = vocab.loc[word]\n",
    "\n",
    "            if verbose:\n",
    "                print(f'({jj-2} + {jj-1}, {jj}): {preceeding_bigram} -> {word} @ {i}, {j}')\n",
    "\n",
    "            transition_mat[i, j] += 1\n",
    "\n",
    "    transition_df = pd.DataFrame(\n",
    "        transition_mat,\n",
    "        index=bigram_vocab.index,\n",
    "        columns=vocab.index\n",
    "    )\n",
    "\n",
    "    return transition_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_direct_bigram_transition_df(tokenized_phrases, vocab, bigram_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VII. Second order sequence model with skips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = [\n",
    "    'check the program log and find out whether it ran please',\n",
    "    'check the battery log and find out whether it ran down please',\n",
    "]\n",
    "tokenized_phrases = [simple_tokenize(phrase) for phrase in phrases]\n",
    "tokenized_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigram_transition_df(\n",
    "    tokenized_phrases,\n",
    "    vocab,\n",
    "    verbose=True,\n",
    "):\n",
    "\n",
    "    columns = vocab.index\n",
    "    index = pd.MultiIndex.from_product([columns, columns])\n",
    "    transition_mat = np.zeros((len(index), len(columns))).astype(int)\n",
    "    transition_df = pd.DataFrame(transition_mat, index=index, columns=columns)\n",
    "\n",
    "    # Loop through and add\n",
    "    for ii, tokens_ii in enumerate(tokenized_phrases):\n",
    "        for jj, word in enumerate(tokens_ii):\n",
    "\n",
    "            # No bigram for first couple words\n",
    "            if jj < 2:\n",
    "                continue\n",
    "\n",
    "            preceeding_word = tokens_ii[jj - 1]\n",
    "\n",
    "            for k, prepreceeding_word in enumerate(tokens_ii[:jj-1]):\n",
    "                prepreceeding_word = tokens_ii[k]\n",
    "\n",
    "                if verbose:\n",
    "                    print(\n",
    "                        f'(({k}, {jj-1}), {jj}): '\n",
    "                        f'({prepreceeding_word}, {preceeding_word}) -> {word}'\n",
    "                    )\n",
    "\n",
    "                transition_df.loc[\n",
    "                    (prepreceeding_word, preceeding_word),\n",
    "                    word\n",
    "                ] += 1\n",
    "\n",
    "    return transition_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(tokenized_phrases)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_df = build_bigram_transition_df(tokenized_phrases, vocab)\n",
    "transition_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ran_votes = transition_df.xs('ran', level=1)\n",
    "ran_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the prediction for the first phrase\n",
    "ran_votes0 = ran_votes.loc[tokenized_phrases[0]] # Should technically be just up to ran, not the full thing\n",
    "ran_votes0_summed = ran_votes0.sum(axis=0)\n",
    "tokenized_phrases[0], ran_votes0_summed, ran_votes0_summed.idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the prediction for the second phrase\n",
    "ran_votes1 = ran_votes.loc[tokenized_phrases[1]] # Should technically be just up to ran, not the full thing\n",
    "ran_votes1_summed = ran_votes1.sum(axis=0)\n",
    "tokenized_phrases[1], ran_votes1_summed, ran_votes1_summed.idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIII. Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful = [('battery', 'ran'),('program', 'ran')]\n",
    "mask = transition_df.index.isin(useful).astype(int)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(transition_df.values * mask.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IX. Rest Stop and an Off Ramp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X. Attention as matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\rm{Attention}(Q, K, V) = \\rm{softmax}\\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$K_{ij} :=$ matrix of masks,\n",
    "where for a given subsequent word $j$ (e.g. down or ran)\n",
    "the value at (i,j) is the relevance (1 or 0)\n",
    "of a preceeding bigram $i$ (e.g. (battery, ran))\n",
    "for voting for that subsequent word.\n",
    "\n",
    "$Q_i$ is the queryâ€”the one-hot feature vector for a single word.\n",
    "Applying it to $K_{ij}$ pulls out the mask of possible relevant bigrams\n",
    "for the single subsequent word.\n",
    "I.e. $Q_i K^{i}_j = F_j$, where the value at $j$ is the relevance of that preceeding bigram\n",
    "for predicting the subsequent word.\n",
    "\n",
    "$V_i$ is the \"values\" matrix.\n",
    "It is yest-to-be understood.\n",
    "\n",
    "Extending to multiple subsequent words (queries; not clear why we have multiple queries yet),\n",
    "$Q_{ik}$ is the one-hot feature vector $Q_i$ for a selected subsequent word $k$.\n",
    "So $Q_{ik} K^{i}_j = F_{kj}$ is the mask $F_j$ for each subsequent word $k$.\n",
    "\n",
    "The \"attention step\" enables us to retrieve a small collection\n",
    "of relevant preceeding bigrams for a subsequent word.\n",
    "The collection of relevant preceeding bigrams is represented as a mask $F_j$\n",
    "and the subsequent word is represented as a vector $Q_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XI. Second order sequence model as matrix multiplications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
