{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers from Scratch\n",
    "This is a workthrough of [the nice derivation found here.](https://e2eml.school/transformers.html#table_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Words -> numerical representation.\n",
    "\n",
    "Consider \"find my files\":\n",
    "\n",
    "\\begin{matrix}\n",
    "\\rm{files} & \\rm{find} & \\rm{my}\n",
    "\\end{matrix}\n",
    "\n",
    "\\begin{pmatrix}\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "1 & 0 & 0\n",
    "\\end{pmatrix}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas one-hot encoding\n",
    "phrase = pd.Series(['find', 'my', 'files'])\n",
    "encoded_phrase = pd.get_dummies(phrase) # one-hot encoding\n",
    "encoded_phrase.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot product\n",
    "\n",
    "$\\vec{a} \\cdot \\vec{b} \\equiv a_i b_i = c$ (scalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix multiplication\n",
    "\n",
    "$\\bf{A} \\bf{B} \\equiv A_{ij}B_{jk} = C_{ik}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix multiplication as a table lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_matrix = rng.uniform(size=(3, 3))\n",
    "some_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_vec = encoded_phrase['files']\n",
    "np.matmul(some_matrix, one_hot_vec) # Last column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First order sequence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = [\n",
    "    'show me my directories please',\n",
    "    'show me my files please',\n",
    "    'show me my photos please',\n",
    "]\n",
    "tokenized_phrases = [simple_tokenize(phrase) for phrase in phrases]\n",
    "tokenized_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(tokenized_phrases):\n",
    "\n",
    "    # Get all tokens\n",
    "    tokens = set()\n",
    "    for tokens_i in tokenized_phrases:\n",
    "        tokens.update(tokens_i)\n",
    "\n",
    "    # Convert to dictionary\n",
    "    vocab = {}\n",
    "    for i, token in enumerate(sorted(tokens)):\n",
    "        vocab[token] = i\n",
    "\n",
    "    return pd.Series(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(tokenized_phrases)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transition_df(tokenized_phrases, vocab, h=1, verbose=True):\n",
    "\n",
    "    transition_mat = np.zeros((len(vocab), len(vocab))).astype(int)\n",
    "\n",
    "    # Loop through and add\n",
    "    for ii, tokens_ii in enumerate(tokenized_phrases):\n",
    "        for jj, word in enumerate(tokens_ii):\n",
    "            start = max(0, jj - h)\n",
    "            for k, other_word in enumerate(tokens_ii[start:jj]):\n",
    "\n",
    "                if verbose:\n",
    "                    print(f'({start}, {jj}): {other_word} -> {word}')\n",
    "\n",
    "                i = vocab.loc[other_word]\n",
    "                j = vocab.loc[word]\n",
    "                transition_mat[i, j] += 1\n",
    "\n",
    "    transition_df = pd.DataFrame(\n",
    "        transition_mat,\n",
    "        index=vocab.index,\n",
    "        columns=vocab.index\n",
    "    )\n",
    "\n",
    "    return transition_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_df = build_transition_df(tokenized_phrases, vocab)\n",
    "transition_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilities\n",
    "transition_df.div(transition_df.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_to_one_hot(vocab):\n",
    "    mat = np.identity(len(vocab), dtype=int)\n",
    "    return pd.DataFrame(mat, index=vocab.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_identity = vocab_to_one_hot(vocab)\n",
    "vocab_identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(transition_df.T.values, vocab_identity.loc['my'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second order sequence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = [\n",
    "    'check whether the battery ran down please',\n",
    "    'check whether the program ran please'\n",
    "]\n",
    "tokenized_phrases = [simple_tokenize(phrase) for phrase in phrases]\n",
    "tokenized_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ngram_vocab(phrases, *args, **kwargs):\n",
    "\n",
    "    vectorizer = CountVectorizer(*args, **kwargs)\n",
    "    X = vectorizer.fit_transform(phrases)\n",
    "    vectorized_phrases = pd.DataFrame(\n",
    "        X.toarray(),\n",
    "        columns=vectorizer.get_feature_names_out(),\n",
    "    )\n",
    "\n",
    "    vocab = pd.Series(vectorizer.vocabulary_).sort_values()\n",
    "\n",
    "    return vectorized_phrases, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phrases, bigram_vocab = build_ngram_vocab(phrases, ngram_range=(2, 2))\n",
    "bigram_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(tokenized_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, bigram_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigram_transition_df(\n",
    "    tokenized_phrases,\n",
    "    vocab,\n",
    "    bigram_vocab,\n",
    "    verbose=True,\n",
    "):\n",
    "\n",
    "    transition_mat = np.zeros((len(bigram_vocab), len(vocab))).astype(int)\n",
    "\n",
    "    # Loop through and add\n",
    "    for ii, tokens_ii in enumerate(tokenized_phrases):\n",
    "        for jj, word in enumerate(tokens_ii):\n",
    "\n",
    "            # No bigram for first word\n",
    "            if jj < 2:\n",
    "                continue\n",
    "\n",
    "            preceeding_bigram = ' '.join([tokens_ii[jj - 2], tokens_ii[jj - 1]])\n",
    "\n",
    "            i = bigram_vocab.loc[preceeding_bigram]\n",
    "            j = vocab.loc[word]\n",
    "\n",
    "            if verbose:\n",
    "                print(f'({jj-2} + {jj-1}, {jj}): {preceeding_bigram} -> {word} @ {i}, {j}')\n",
    "\n",
    "            transition_mat[i, j] += 1\n",
    "\n",
    "    transition_df = pd.DataFrame(\n",
    "        transition_mat,\n",
    "        index=bigram_vocab.index,\n",
    "        columns=vocab.index\n",
    "    )\n",
    "\n",
    "    return transition_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_bigram_transition_df(tokenized_phrases, vocab, bigram_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second order sequence model with skips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = [\n",
    "    'check the program log and find out whether it ran please',\n",
    "    'check the battery log and find out whether it ran down please',\n",
    "]\n",
    "tokenized_phrases = [simple_tokenize(phrase) for phrase in phrases]\n",
    "tokenized_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(tokenized_phrases)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_df = build_transition_df(tokenized_phrases, vocab, h=100)\n",
    "transition_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
